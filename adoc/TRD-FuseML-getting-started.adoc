:docinfo:

// = {title}
= {centralProduct} with {K8s} - Getting Started

// SUSE {K8s} - FuseML
// :author: Alex Arnoldy
:revnumber: 0.0.1
:toc2:
:toc-title: FuseML with {K8s} - Getting Started

:toclevels: 4

:sles: SUSE Linux Enterprise Server

:centralProduct: FuseML
:rancher: SUSE Rancher
:company: SUSE
:operatingSystem: openSUSE JeOS 15.3
:osURL: https://get.opensuse.org/leap/
:K8s: K3s

== Motivation

When {company} set out to create a new, indispensible AI/ML tool for its customers, we were certain that we didnâ€™t want to create just another framework/training/analysis tool. What the wild west of AI/ML needs more than anything else is something that both empowers AI practicioners to use the right tool for the job at hand, as well as removes the friction associated with integrating these tools into the larger AI/ML workflow. 

The defining characteristic of {centralProduct} is that it provides the "glue" (aka integration logic) to fuse together the inputs and outputs of various AI/ML tools that are applied at different stages of the AI/ML workflow. Not only does this mean each team can use their favorite tool to accomplish their portion of the workflow, but it also allows the flexibility to change tools based on the stage of the project. 

For example, early an ML project's lifecycle Data Scientists will often rely on tools that provide deeper introspection at the cost of performance. As well, MLOps engineers might prefer automation tools that provide break points for them to evaluate the process at various stages. They might also leverage presentation software that is free and easy to use. When a project is ready for production, the need will shift towards tools that emphasize performance and automation.



== Technical overview

FuseML can run locally, on its own Kubernetes cluster, or alongside an AI/ML workload on a shared Kubernetes cluster. In this design we will use {rancher} to deploy FuseML in SUSE {K8s}. {K8s} uses a cutting edge architecture which adapts the prinicpals of containerization for Kuberetes cluster itself. {K8s} deploys as a single, static binary with all dependencies handled inside. Not only does this create fantastic portability, it also significantly enhances security. This design is part of what allows {K8s} to be FIPS compliant with no modifications.

FuseML will be deployed into its own {rancher} Project, alongside the machine learning workload it will deploy and enhance. Using {rancher} projects further improves security by separating administrative access from user access as well as allowing the application of resource minimums and limits that ensure the highest, consistent performance possible.

For the simplest deployment we will use two virtual machines with the following configurations:

* name "suse-rancher-VM":
** 2 vCPUs, 4 GB RAM, 10GB virtual disk
** Virtual bridge/NAT network with access to the Internet
** {operatingsystem}
** {rancher} v2.6 running on Docker CE

* name "fuseml-k3s-VM":
** 4+ vCPUs, 8+ GB RAM, 20GB virtual disk
** Virtual bridge/NAT network with access to the Internet
** {operatingsystem}
** Latest release of {K8s}

This configuration can be deployed on any system or systems that meet the minimum hardware requirements. A system with 16GB of total RAM could be used as long as there were very few other tasks running at the same time. The VMs do not need to be on the same network but they do need to be able access the Internet as well as each other. In addition, the user needs to be able to access the VMs through a web browser. 

NOTE: Minimally configured systems might experience transient errors during high CPU load. These may take the form of timeout errors or "dial tcp <IP address>:<port>: connect: no route to host" errors. These are almost always non-fatal and simply re-running the command will very likely lead to a successful completion of the command.

== Running {operatingsystem}

{operatingsystem} ({osurl}) is a minimalized, virtualization specific edition of {sles} that runs as a pre-installed operating system. Pre-configured images are available for KVM, Xen, VMware, OpenStack, and Hyper-V. The OpenStack image can be run on KVM or Xen but allows the use of cloud-init for customization during the initial boot sequence.

For this project we have chosen the KVM {operatingsystem} qcow2 image to allow for simple configuration. If cloud-init infrastructure is available, the OpenStack qcow2 image can be used for even easier and repeatable configuration.

TIP: After downloading the {operatingsystem} image, we renamed it to match the hostname of the first VM (suse-rancher-VM in our case), then made a copy of it, naming the second copy appropriate for the second VM (fuseml-{k8s}-VM in our case). The initial boot of the {operatingsystem} image will invoke a configuration tool to customize the image. Making two copies of the image before attaching them to VMs allowed individualizing each VM.

It is assumed that IP management is handled with a DHCP server or manually by the user during the initial boot process.

TIP: After booting and configuring each VM, snapshot it to make it easier to restart the process or try different configurations.

NOTE: Since the {operatingsystem} image configures the root user account during the initial boot, the remainder of this process assumes the commands are run as the root user.



== Setting up SUSE Rancher

For this implentation we will run {rancher} on Docker Community Edition. While this cannot be used in production due to limitations in flexibility and performance, the installation process is extremely easy:

* Install and start Docker CE on the first VM:
[source,bash]
----
zypper -n in docker
systemctl enable --now docker
docker ps -a
----
** The output from the final command should complete without error and show the headers for the command, starting with "CONTAINER ID"

IMPORTANT: The command below will cause the {rancher} server to generate self-signed certificates. Additional options can be used to include pre-existing TLS certificates or certificates issued by Let's Encrypt. See the appropriate documentation for more information.

* Run the {rancher} server container:
[source,bash]
----
docker run \
	--detach \
	--restart=unless-stopped \
	--publish 80:80 --publish 443:443 \
	--privileged \
  --name rancher \
  -e CATTLE_BOOTSTRAP_PASSWORD=FuseML4Me! \
  rancher/rancher 
----

* After the container has been running for about two minutes, execute the following command to get the bootstrap password:
[source,bash]
----
docker logs rancher 2>&1 | grep "Bootstrap Password"
----

NOTE: It can take a few minutes for the {rancher} container to present a web page. When it does, it will show a warning that the certificate is untrusted. Indicate that you understand the risks and continue.

* Use a web browser to access the IP address of the first VM on port 443

NOTE: IF the virtual machines are behind a NAT router it may be necessary to configure port forwarding for ports 80 and 443 to the first VM. Additionally, an SSH tunnel can be created to access the virtual machines. In our case we used an SSH tunnel from our workstation (with the browser) through the KVM host to access ports 80 and 443 on the first VM. The command we used was `ssh -L localhost:8080:10.236.1.11:80 -L localhost:4443:10.236.1.11:443 -N sles@kvm-host1.sandbox.local`. This allowed us to browse the {rancher} server at https://localhost:4443.

** Use the bootstrap password to log into the {rancher} UI and follow the instructions to set a specific Admin password.

CAUTION: Make sure the "Server URL" uses https://, has the resolvable hostname or IP address of the first VM and does not indicate a port.


=== Installing {K8s}

.Run the following commands the second VM

NOTE: Newer versions of containerd require AppArmor to be installed for certain operating systems. Here were are using SLES JeOS, which has AppArmor available but not fully installed. AppArmor isn't needed here so we will disable it.

////
* Install AppArmor:
[source,bash]
----
zypper in -t pattern apparmor
----
////

NOTE: Some Kubernetes features can be negatively affected by not having a unique hostname associated with an external IP address.

* (Optional) If a hostname isn't already set in the /etc/hosts file and associated with an external IP address, update the /etc/hosts file so one line contains the external IP address, followed by the FQDN, followed by the hostname. For example:
----
172.16.240.91	  fuseml-k3s-vm.sandbox.local   fuseml-k3s-vm
----
////
HOSTNAME="fuseml-k3s-vm.sandbox.local fuseml-k3s-vm"
IPADDRESS=
sed -i "/127.0.0.1/ s/localhost/localhost ${HOSTNAME}/" /etc/hosts
hostnamectl set-hostname fuseml-k3s-vm
////

* Verify that AppArmor is showing as enabled (even though it's not fully installed):
[source,bash]
----
cat /sys/module/apparmor/parameters/enabled
----
** Returns "Y" if AppArmor is considered to be enabled

* Disable AppArmor:
[source,bash]
----
sed -i '/GRUB_CMDLINE_LINUX_DEFAULT/ s/quiet/quiet apparmor=0/' /etc/default/grub
grub2-mkconfig -o /boot/grub2/grub.cfg
reboot
----

* After the VM has rebooted, ensure AppArmor is now showing as not enabled 
[source,bash]
----
cat /sys/module/apparmor/parameters/enabled
----
** Returns "N" 

* Download and install {K8s}:
[source,bash] 
----
curl -sfL https://get.k3s.io | sh -s - --disable=traefik 
#curl -sfL https://get.k3s.io | sh -s - --disable=traefik --node-name=fuseml-k3s-vm 
----

* Monitor the progress of the installation: `watch -c "kubectl get deployments -A"`
** The {k8s} deployment is complete when elements of all the deployments (coredns, local-path-provisioner, and metrics-server) show at least "1" as "AVAILABLE"
** Use Ctrl+c to exit the watch loop after all deployment pods are running

* While the kubectl tool built into {k8s} knows where to find the KUBECONFIG file for this cluster, other utilities don't. Create a link from the KUBECONFIG file to the location it is normally found:
[source,bash]
----
ln -s /etc/rancher/k3s/k3s.yaml .kube/config
----


== Deploy MetalLB load balancer

* Pull and apply the MetalLB manifests:

[source,bash] 
----
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/metallb.yaml
# On first install only
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
----

NOTE: A static IP address normally falls outside the range of IP addresses that a DHCP server will assign. If this is running in a private environment (i.e. a laptop), and there aren't IP static addresses available, you can pick an IP address at the top of the subnet, verify it is not in use, and use it as your static IP address.

* Provide at least one static IP address to MetalLB (if only providing one IP address, set that IP as both DEFAULT_IP_RANGE_START and DEFAULT_IP_RANGE_END):

[source,bash] 
----
export DEFAULT_IP_RANGE_START=
export DEFAULT_IP_RANGE_END=
----

* Create the MetalLB configuration file for layer 2 routing. See https://metallb.universe.tf/configuration/ for other routing options and https://raw.githubusercontent.com/google/metallb/v0.9.3/manifests/example-config.yaml for lots of configuration options

[source,bash] 
----
cat <<EOF> metallb-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - ${DEFAULT_IP_RANGE_START}-${DEFAULT_IP_RANGE_END}
EOF
----

* Create configmap: `kubectl apply -f metallb-config.yaml`

* Verify the configuration was applied correctly (especially review the IP address pool): `kubectl get configmap config -n metallb-system -o yaml`

* Verify the MetalLB load balancer is running: `kubectl get all -n metallb-system`

////
From the Home screen, click the three horizontal (aka Hamburger Menu) lines in the top left corner

Under Global Apps, select Cluster Management
Click Create at the top right
Move the slider from "RKE1" to "RKE2/K3s"
Click the "Create Custom Cluster" button
Name the cluster (fuseml-rke2 in our case)
Click the Create button at the bottom of the screen

CAUTION: If {rancher} server was installed with a self-signed certificate, check the box labeled "Insecure: Select this to skip TLS verification if your server has a self-signed certificate."
Copy the Registration Command under Step 2
As root, paste the command onto the command line for the second VM
////


=== Installing the FuseML Installer utility

* The fuseml-installer uses helm and git, so install them first:
[source,bash]
----
zypper in helm git
----

* Download and run the fuseml-installer utility:
[source,bash]
----
curl -sfL https://raw.githubusercontent.com/fuseml/fuseml/main/install.sh | sh -
----

=== Installing FuseML 

* Verify the version of the of the fuseml-installer:
[source,bash]
----
fuseml-installer version
----

* Install FuseML on your {k8s} cluster:
[source,bash]
----
fuseml-installer install
----

NOTE: This command will go through several steps to install of all of FuseML components. The entire process should take less than 15 minutes. 

IMPORTANT: At the end of the installer output will be a statement that provides the URL for the FuseML server, such as `export FUSEML_SERVER_URL=http://fuseml-core.172.16.240.91.nip.io`. This URL requires name resolution from a specific Internet DNS server. If this VM cannot reach the DNS server, you can add the gitea.w.x.y.z.nip.io, tekton.w.x.y.z.nip.io, and fuseml-core.w.x.y.z.nip.io URLs to the /etc/hosts file.

* Verify the VM can resolve the URLs for the FuseML services with the `getent hosts` command:

[source,bash]
----
export FUSEML_SERVER=$(kubectl get VirtualService -n fuseml-core fuseml-core -o jsonpath="{.spec.hosts[0]}")
getent hosts ${FUSEML_SERVER}
----

* Place the `export FUSEML_SERVER_URL` command from the installation output into the ~/.bashrc file and source the file:

[source,bash]
----
echo "export FUSEML_SERVER_URL=http://${FUSEML_SERVER}" >> ~/.bashrc
source ~/.bashrc
----

NOTE: After a successfull installation, the `fuseml` command line tool will be left in the current working directory. For ease of use, it's a good idea to move it to a directory in your PATH, such as /usr/local/bin.

* Move the `fuseml` tool into /usr/local/bin:
[source,bash]
----
mv fuseml /usr/local/bin/
----

== Running a basic workflow based on MLFlow and KFServing

NOTE: The FuseML Tutorials section of the https://fuseml.github.io/docs/ website features an excellent example of creating a machine learning workflow based on MLFlow and KFSErving. This section will simply add context and clarification to that process, where ever needed.


.Before beginning the tutorial, verify that the `fuseml` tool can create the target {k8s} cluster:
[source,bash]
----
fuseml version
----
* Version information should be returned for the client (the `fuseml` tool) and the server (the FuseML application running on the {k8s} cluster), similar to this:
----
client:
  version: v0.2.1
  gitCommit: 3a4bd7f9
  buildDate: 2021-09-15T21:38:26Z
  goVersion: go1.17
  compiler: gc
  platform: linux/amd64
server:
  version: v0.2.1
  gitcommit: 3a4bd7f9
  builddate: 2021-09-15T21:33:20Z
  golangversion: go1.16.8
  golangcompiler: gc
  platform: linux/amd64
----
* A few things might cause the server to fail to return the correct information. If this is the case, verify:
** The `kubectl get nodes -o wide` command returns the information for the correct {k8s} cluster 
** The command `echo ${FUSEML_SERVER_URL}` returns the URL for the fuseml-core service, such as "http://fuseml-core.10.111.8.101.nip.io"

.If the {k8s} cluster has minimal resources, consider adding the MLFlow and KFServing extensions seperately, for example:
[source,bash]
----
fuseml-installer extensions --add mlflow
fuseml-installer extensions --add kfserving
----

.Take note that there are additional example workflows in the codesets/mlflow/ directory that can be run after completing the first one.

.The `fuseml codeset register` command loads the project files into Gitea as the source artifact for the workflow. Have a closer look at the codesets/mlflow/sklearn/ directory to learn more.

.FuseML Workflow runs are highly dependent on CPU and memory resources. A system configured with minimal resources will take noticeably longer to complete the example runs.




== Summary

////
MongoDB with SUSE Rancher makes it possible to easily build, deploy, and manage resilient, scalable data services.


== Additional resources

For more information, visit: 

*	https://rancher.com/docs/rancher/v2.x/en/best-practices/[Rancher best practices guide]
*	https://rancher.com/docs/rancher/v2.x/en/troubleshooting/[Rancher troubleshooting tips]
*	https://github.com/mongodb/mongodb-kubernetes-operator[MongoDB best practices]

++++
<?pdfpagebreak?>
++++

:leveloffset: 0
// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// include::common_gfdl1.2_i.adoc[]

:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end

////