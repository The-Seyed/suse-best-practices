:docinfo:

// = {title}
= {centralProduct} with {K8s} - Getting Started

// SUSE {K8s} - FuseML
// :author: Alex Arnoldy
:revnumber: 0.0.1
:toc2:
:toc-title: FuseML with {K8s} - Getting Started

:toclevels: 4

:sles: SUSE Linux Enterprise Server

:centralProduct: FuseML
:rancher: SUSE Rancher
:company: SUSE
:operatingSystem: openSUSE JeOS 15.3
:osURL: https://get.opensuse.org/leap/
:K8s: K3s

== Motivation

When {company} set out to create a new, indispensible AI/ML tool for its customers, we were certain that we didn’t want to create just another framework/training/analysis tool. What the wild west of AI/ML needs more than anything else is something that both empowers AI practicioners to use the right tool for the job at hand, as well as removes the friction associated with integrating these tools into the larger AI/ML workflow. 

The defining characteristic of {centralProduct} is that it provides the "glue" (aka integration logic) to fuse together the inputs and outputs of various AI/ML tools that are applied at different stages of the AI/ML workflow. Not only does this mean each team can use their favorite tool to accomplish their portion of the workflow, but it also allows the flexibility to change tools based on the stage of the project. 

For example, early an ML project's lifecycle Data Scientists will often rely on tools that provide deeper introspection at the cost of performance. As well, MLOps engineers might prefer automation tools that provide break points for them to evaluate the process at various stages. They might also leverage presentation software that is free and easy to use. When a project is ready for production, the need will shift towards tools that emphasize performance and automation.


////
Agility is the name of the game in modern application development.  This is driving developers toward more agile, 
cloud native methodologies that focus on microservices architectures and streamlined workflows.  
Container technologies, like Kubernetes, embody this agile approach and help enable cloud native transformation.

SUSE Rancher simplifies Kubernetes management, empowering you to take control of your IT landscape and create 
an agile data platform that accelerates achievement of your goals.  
Rancher enables you to manage dynamic, robust, multi-cluster Kubernetes environments and supports any 
CNCF-certified Kubernetes distribution.  With built-in resilience and scalability, unified security and policy management, 
and a rich catalog of shared tools and services, Rancher helps you accelerate development-to-production and innovate everywhere.

MongoDB is a cloud native database technology that brings all the power of the traditional, 
relational database and provides a more natural way to work with modern data.  
MongoDB offers a variety of compelling features that make it a natural partner for a SUSE Rancher agile data platform.  
These include:

Flexible Data Model:: MongoDB’s dynamic schema is ideal for handling changing requirements and continuous delivery.  
You can seamlessly roll out new features without having to update existing records — a process that can take weeks for traditional, 
relational databases.  DevOps teams can quickly model data against an ever-changing environment and roll these changes into production. 
This results in faster time to market and faster time to value.

Resilience:: MongoDB’s replica sets have built-in redundancy, providing greater resilience and enhancing disaster recovery capabilities.  
Administrators can even isolate operational workloads from analytical reporting in single database cluster to ensure sufficient 
resources are allocated to handle demand.

Monitoring and Automation::  Heterogeneous services increase the level of complexity and can stall productivity. 
Technology that handles monitoring and automation is critical to keeping DevOps teams productive as their environments evolve. 
MongoDB Ops Manager features visualization, custom dashboards, and automated alerting.  It tracks, reports, processes, 
and visualizes 100+ key database and systems-health metrics, including operations counters, CPU utilization, replication status, 
and node status.

Scalability:: MongoDB’s auto-sharding automatically partitions and distributes the database across nodes, serving IT infrastructures 
that require dynamic, high-performance capabilities.  Distribution can even span different geographic regions.  
MongoDB is ideally suited to scale-out architectures.


With enterprise-grade products, proactive support, and success-focused services and training, SUSE and MongoDB deliver 
an agile data platform that helps organizations achieve their cloud native goals.
////

== Technical overview

FuseML can run locally, on its own Kubernetes cluster, or alongside an AI/ML workload on a shared Kubernetes cluster. In this design we will use {rancher} to deploy FuseML in SUSE {K8s}. {K8s} uses a cutting edge architecture which adapts the prinicpals of containerization for Kuberetes cluster itself. {K8s} deploys as a single, static binary with all dependencies handled inside. Not only does this create fantastic portability, it also significantly enhances security. This design is part of what allows {K8s} to be FIPS compliant with no modifications.

FuseML will be deployed into its own {rancher} Project, alongside the machine learning workload it will deploy and enhance. Using {rancher} projects further improves security by separating administrative access from user access as well as allowing the application of resource minimums and limits that ensure the highest, consistent performance possible.

For the simplest deployment we will use two virtual machines with the following configurations:

* name "suse-rancher-VM":
** 2 vCPUs, 4 GB RAM, 10GB virtual disk
** Virtual bridge/NAT network with access to the Internet
** {operatingsystem}
** {rancher} v2.6 running on Docker CE

* name "fuseml-k3s-VM":
** 4 vCPUs, 8 GB RAM, 20GB virtual disk
** Virtual bridge/NAT network with access to the Internet
** {operatingsystem}
** Latest release of {K8s}

This configuration can be deployed on any system or systems that meet the minimum hardware requirements. A system with 16GB of total RAM could be used as long as there were very few other tasks running at the same time. The VMs do not need to be on the same network but they do need to be able access the Internet as well as each other. In addition, the user needs to be able to access the VMs through a web browser. 

== Running {operatingsystem}

{operatingsystem} ({osurl}) is a minimalized, virtualization specific edition of {sles} that runs as a pre-installed operating system. Pre-configured images are available for KVM, Xen, VMware, OpenStack, and Hyper-V. The OpenStack image can be run on KVM or Xen but allows the use of cloud-init for customization during the initial boot sequence.

For this project we have chosen the KVM {operatingsystem} qcow2 image to allow for simple configuration. If cloud-init infrastructure is available, the OpenStack qcow2 image can be used for even easier and repeatable configuration.

TIP: After downloading the {operatingsystem} image, we renamed it to match the hostname of the first VM (suse-rancher-VM in our case), then made a copy of it, naming the second copy appropriate for the second VM (fuseml-{k8s}-VM in our case). The initial boot of the {operatingsystem} image will invoke a configuration tool to customize the image. Making two copies of the image before attaching them to VMs allowed individualizing each VM.

It is assumed that IP management is handled with a DHCP server or manually by the user during the initial boot process.

TIP: After booting and configuring each VM, snapshot it to make it easier to restart the process or try different configurations.

NOTE: Since the {operatingsystem} image configures the root user account during the initial boot, the remainder of this process assumes the commands are run as the root user.



== Setting up SUSE Rancher

For this implentation we will run {rancher} on Docker Community Edition. While this cannot be used in production due to limitations in flexibility and performance, the installation process is extremely easy:

* Install and start Docker CE on the first VM:
[source,bash]
----
zypper -n in docker
systemctl enable --now docker
docker ps -a
----
** The output from the final command should complete without error and show the headers for the command, starting with "CONTAINER ID"

IMPORTANT: The command below will cause the {rancher} server to generate self-signed certificates. Additional options can be used to include pre-existing TLS certificates or certificates issued by Let's Encrypt. See the appropriate documentation for more information.

* Run the {rancher} server container:
[source,bash]
----
docker run \
	--detach \
	--restart=unless-stopped \
	--publish 80:80 --publish 443:443 \
	--privileged \
  --name rancher \
  -e CATTLE_BOOTSTRAP_PASSWORD=FuseML4Me! \
  rancher/rancher 
----

* After the container has been running for about two minutes, execute the following command to get the bootstrap password:
[source,bash]
----
docker logs rancher 2>&1 | grep "Bootstrap Password"
----

NOTE: It can take a few minutes for the {rancher} container to present a web page. When it does, it will show a warning that the certificate is untrusted. Indicate that you understand the risks and continue.

* Use a web browser to access the IP address of the first VM on port 443

NOTE: IF the virtual machines are behind a NAT router it may be necessary to configure port forwarding for ports 80 and 443 to the first VM. Additionally, an SSH tunnel can be created to access the virtual machines. In our case we used an SSH tunnel from our workstation (with the browser) through the KVM host to access ports 80 and 443 on the first VM. The command we used was `ssh -L localhost:8080:10.236.1.11:80 -L localhost:4443:10.236.1.11:443 -N sles@kvm-host1.sandbox.local`. This allowed us to browse the {rancher} server at https://localhost:4443.

** Use the bootstrap password to log into the {rancher} UI and follow the instructions to set a specific Admin password.

CAUTION: Make sure the "Server URL" uses https://, has the resolvable hostname or IP address of the first VM and does not indicate a port.


=== Installing {K8s}

.Run the following commands the second VM

NOTE: Newer versions of containerd require AppArmor to be installed for certain operating systems. Here were are using SLES JeOS, which has AppArmor available but not fully installed. AppArmor isn't needed here so we will disable it.

////
* Install AppArmor:
[source,bash]
----
zypper in -t pattern apparmor
----
////

HOSTNAME="fuseml-k3s-vm.sandbox.local fuseml-k3s-vm"
IPADDRESS=
sed -i "/127.0.0.1/ s/localhost/localhost ${HOSTNAME}/" /etc/hosts
hostnamectl set-hostname fuseml-k3s-vm

* Verify that AppArmor is showing as enabled (even though it's not fully installed):
[source,bash]
----
cat /sys/module/apparmor/parameters/enabled
----
** Returns "Y" if AppArmor is considered to be enabled

* Disable AppArmor:
[source,bash]
----
sed -i '/GRUB_CMDLINE_LINUX_DEFAULT/ s/quiet/quiet apparmor=0/' /etc/default/grub
grub2-mkconfig -o /boot/grub2/grub.cfg
reboot
----

* After the VM has rebooted, ensure AppArmor is now showing as not enabled 
[source,bash]
----
cat /sys/module/apparmor/parameters/enabled
----
** Returns "N" 

* Download and install {K8s}:
[source,bash] 
----
curl -sfL https://get.k3s.io | sh -s - --disable=traefik 
#curl -sfL https://get.k3s.io | sh -s - --disable=traefik --node-name=fuseml-k3s-vm 
----

* Monitor the progress of the installation: `watch -c "kubectl get deployments -A"`
** The {k8s} deployment is complete when elements of all the deployments (coredns, local-path-provisioner, and metrics-server) show at least "1" as "AVAILABLE"
** Use Ctrl+c to exit the watch loop after all deployment pods are running

* While the kubectl tool built into {k8s} knows where to find the KUBECONFIG file for this cluster, other utilities don't. Create a link from the KUBECONFIG file to the location it is normally found:
[source,bash]
----
ln -s /etc/rancher/k3s/k3s.yaml .kube/config
----


== Deploy MetalLB load balancer

* Pull and apply the MetalLB manifests:

[source,bash] 
----
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/metallb.yaml
# On first install only
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
----

NOTE: A static IP address normally falls outside the range of IP addresses that a DHCP server will assign. If this is running in a private environment (i.e. a laptop), and there aren't IP static addresses available, you can pick an IP address at the top of the subnet, verify it is not in use, and use it as your static IP address.

* Provide at least one static IP address to MetalLB (if only providing one IP address, set that IP as both DEFAULT_IP_RANGE_START and DEFAULT_IP_RANGE_END):

[source,bash] 
----
export DEFAULT_IP_RANGE_START=
export DEFAULT_IP_RANGE_END=
----

* Create the MetalLB configuration file for layer 2 routing. See https://metallb.universe.tf/configuration/ for other routing options and https://raw.githubusercontent.com/google/metallb/v0.9.3/manifests/example-config.yaml for lots of configuration options

[source,bash] 
----
cat <<EOF> metallb-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - ${DEFAULT_IP_RANGE_START}-${DEFAULT_IP_RANGE_END}
EOF
----

* Create configmap: `kubectl apply -f metallb-config.yaml`

* Verify the configuration was applied correctly (especially review the IP address pool): `kubectl get configmap config -n metallb-system -o yaml`

* Verify the MetalLB load balancer is running: `kubectl get all -n metallb-system`

////
From the Home screen, click the three horizontal (aka Hamburger Menu) lines in the top left corner

Under Global Apps, select Cluster Management
Click Create at the top right
Move the slider from "RKE1" to "RKE2/K3s"
Click the "Create Custom Cluster" button
Name the cluster (fuseml-rke2 in our case)
Click the Create button at the bottom of the screen

CAUTION: If {rancher} server was installed with a self-signed certificate, check the box labeled "Insecure: Select this to skip TLS verification if your server has a self-signed certificate."
Copy the Registration Command under Step 2
As root, paste the command onto the command line for the second VM
////


=== Installing FuseML

* The fuseml-installer uses helm and git, so install them first:
[source,bash]
----
zypper in helm git
----

* Download and run the fuseml-installer utility:
[source,bash]
----
curl -sfL https://raw.githubusercontent.com/fuseml/fuseml/main/install.sh | sh -
----

=== Deploying FuseML

fuseml-installer install

////
When the Kubernetes cluster is running and storage is configured, it is time to deploy a highly available MongoDB database.

MongoDB resources are created in Kubernetes as custom resources. After you create or update a MongoDB Kubernetes resource specification, 
you direct MongoDB Kubernetes Operator to apply this specification to your Kubernetes environment. 
Kubernetes Operator creates the defined StatefulSets, services and other Kubernetes resources. 
After the Operator finishes creating those objects, it updates the Ops Manager deployment configuration to reflect changes.

The following example shows a resource specification for a 
https://docs.mongodb.com/manual/reference/glossary/#term-replica-set[replica set] configuration:

[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDB
metadata:
  name: my-replica-set
spec:
  members: 3
  version: "4.2.2-ent"
  service: my-service
  opsManager: # Alias of cloudManager
    configMapRef:
      name: my-project
  credentials: my-credentials
  persistent: true
  type: ReplicaSet
  podSpec:
    cpu: "0.25"
    memory: "512M"
    persistence:
      multiple:
        data:
          storage: "10Gi"
        journal:
          storage: "1Gi"
          labelSelector:
            matchLabels:
              app: "my-app"
        logs:
          storage: "500M"
          storageClass: standard
    podAntiAffinityTopologyKey: nodeId
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
    podTemplate:
      metadata:
        labels:
          label1: mycustomlabel
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  topologyKey: "mykey"
                weight: 50
  security:
    tls:
      enabled: true
    authentication:
      enabled: true
      modes: ["X509"]
      internalCluster: "X509"
  additionalMongodConfig:
    net:
      ssl:
        mode: preferSSL
----

Full details can be found https://docs.mongodb.com/kubernetes-operator/master/reference/k8s-operator-specification[here].
////


== Creating a persistent volume

////
You can now create a persistent volume claim (PVC) based on the storage class. Dynamic provisioning will be created without explicitly 
provisioning a persistent volume (PV). As part of deployment, the Kubernetes operator creates 
https://kubernetes.io/docs/concepts/storage/persistent-volumes[persistent volumes] for the Ops Manager StatefulSets. 
The Kubernetes container uses persistent volumes to maintain the cluster state between restarts.
////


== Deploying MongoDB Kubernetes Operator

////
Kubernetes needs help creating and managing stateful applications like databases.  The typical lifecycle events of a MongoDB cluster may 
include provisioning storage and computing power, configuring network connections, setting up users, and more.  
This is where the MongoDB Enterprise Kubernetes Operator comes in.  It translates the human knowledge of how to create a MongoDB 
instance into a scalable, repeatable, and standardized methodology.  And it does this by using the built-in Kubernetes API and tools.
To use the operator, you simply need to provide it with the specifications for your MongoDB cluster.  
The operator uses this information to direct Kubernetes into performing all the required steps to achieve the end state.

The general commands for deploying the MongoDB Enterprise Operator are:

[source,bash]
----
kubectl describe deployments mongodb-enterprise-operator -n <namespace>


helm install <chart-name> helm_chart \
     --values helm_chart/values.yaml \
----


The next step after deploying the operator is to create the database using a `yaml` file, such as:

[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDB
metadata:

  name: <my-standalone>

spec:

  version: "4.2.2-ent"

  opsManager:
    configMapRef:

      name: <configMap.metadata.name>

            # Must match metadata.name in ConfigMap file

  credentials: <mycredentials>

  type: Standalone
  persistent: true
----


Now, you can deploy the database and check the status of the deployment with:

[source,bash]
----
kubectl apply -f <standalone-conf>.yaml

kubectl get mdb <resource-name> -o yaml
----



At this point, MongoDB has been deployed via the operator. Additional settings can be applied to create 
https://docs.mongodb.com/kubernetes-operator/stable/tutorial/deploy-replica-set/[replica] sets to further enhance data availability.  
Also, sharded clusters can be created to enable greater throughput across a distributed system.

Additionally, see specific https://github.com/mongodb/mongodb-enterprise-kubernetes[documentation] and steps for full installation 
and configuration options.
////


== Deploying MongoDB Ops Manager resource

////
Kubernetes needs help creating and managing stateful applications like databases.  The typical lifecycle events of a MongoDB cluster may 
include provisioning, storage and compute. Ops Manager can be deployed via the MongoDB Kubernetes Operator to manage MongoDB resources 
in a cluster.  The Operator manages the lifecycle of each of these deployments differently.
The Operator manages Ops Manager deployments using the Ops Manager custom resource. The Operator watches the custom resource’s 
specification for changes. When the specification changes, the Operator validates the changes and makes the appropriate updates to the 
resources in the cluster.
Ops Manager custom resources specification defines the following components: Application Database, Ops Manager application, 
and Backup Daemon. Summarized instructions to create Ops Manager are below, but full details can be found 
https://docs.mongodb.com/kubernetes-operator/master/tutorial/deploy-om-container[here].

To begin, run the following command to execute all `kubectl` commands in the namespace you created:

[source,bash]
----
kubectl config set-context $(kubectl config current-context) --namespace=<namespace>
----

Create Ops Manager object as below:

[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDBOpsManager
metadata:
  name: <myopsmanager>
spec:
  replicas: 1
  version: <opsmanagerversion>
  adminCredentials: <adminusercredentials> # Should match metadata.name
  externalConnectivity:
    type: LoadBalancer
  applicationDatabase:
    members: 3
    version: <mongodbversion>
    persistent: true
----
////


== Loading and querying the database

////
When the database has been created, you can populate it with some sample data.

First, find the pod that is running MongoDB:

[source,bash]
----
POD=`kubectl get pods -l app=mongo | grep Running | grep 1/1 | awk '{print $1}'`
----

Then, access the MongoDB shell on that POD instance:

[source,bash]
----
$ kubectl exec -it $POD mongo
MongoDB shell version v4.0.0
connecting to: mongodb://127.0.0.1:27017
MongoDB server version: 4.0.0
Welcome to the MongoDB shell.
----

Now, using the MongoDB shell, you can populate a collection:

[source,bash]
----
db.ships.insert({name:'USS Enterprise-D',operator:'Starfleet',type:'Explorer',class:'Galaxy',crew:750,codes:[10,11,12]})
db.ships.insert({name:'USS Prometheus',operator:'Starfleet',class:'Prometheus',crew:4,codes:[1,14,17]})
db.ships.insert({name:'USS Defiant',operator:'Starfleet',class:'Defiant',crew:50,codes:[10,17,19]})
db.ships.insert({name:'IKS Buruk',operator:' Klingon Empire',class:'Warship',crew:40,codes:[100,110,120]})
db.ships.insert({name:'IKS Somraw',operator:' Klingon Empire',class:'Raptor',crew:50,codes:[101,111,120]})
db.ships.insert({name:'Scimitar',operator:'Romulan Star Empire',type:'Warbird',class:'Warbird',crew:25,codes:[201,211,220]})
db.ships.insert({name:'Narada',operator:'Romulan Star Empire',type:'Warbird',class:'Warbird',crew:65,codes:[251,251,220]})
----

And you can run some operations on the MongoDB collection.

For example, find one arbitrary document:

[source,bash]
----
db.ships.findOne()
{
	"_id" : ObjectId("5b5c16221108c314d4c000cd"),
	"name" : "USS Enterprise-D",
	"operator" : "Starfleet",
	"type" : "Explorer",
	"class" : "Galaxy",
	"crew" : 750,
	"codes" : [
		10,
		11,
		12
	]
}
----


You can also find ALL documents and apply some basic formatting:

[source,bash]
----
db.ships.find().pretty()
 {
 	"_id" : ObjectId("5b5c16221108c314d4c000d1"),
 	"name" : "IKS Somraw",
 	"operator" : " Klingon Empire",
 	"class" : "Raptor",
 	"crew" : 50,
 	"codes" : [
 		101,
 		111,
 		120
 	]
 }
 {
 	"_id" : ObjectId("5b5c16221108c314d4c000d2"),
 	"name" : "Scimitar",
 	"operator" : "Romulan Star Empire",
 	"type" : "Warbird",
 	"class" : "Warbird",
 	"crew" : 25,
 	"codes" : [
 		201,
 		211,
 		220
 	]
 }
----


And you can get a list of the names of the ships:

[source,bash]
----
 db.ships.find({}, {name:true, _id:false})
 { "name" : "USS Enterprise-D" }
 { "name" : "USS Prometheus" }
 { "name" : "USS Defiant" }
 { "name" : "IKS Buruk" }
 { "name" : "IKS Somraw" }
 { "name" : "Scimitar" }
 { "name" : "Narada" }
----
////


== Simulating node failure & restoration

////
Next, simulate a node failure by cordoning off the node on which MongoDB is running:

[source,bash]
----
$ NODE=`kubectl get pods -l app=mongo -o wide | grep -v NAME | awk '{print $7}'`

$ kubectl cordon ${NODE} node/ip-172-31-29-132.compute.internal cordoned
----

The above command disables scheduling on one of the nodes.
Check this with:

[source,bash]
----
$ kubectl get nodes
NAME                                           STATUS                     ROLES               AGE   VERSION
ip-172-31-24-121.compute.internal   Ready                      worker              47h   v1.13.4
ip-172-31-26-49.compute.internal    Ready                      controlplane,etcd   47h   v1.13.4
ip-172-31-28-65.compute.internal    Ready                      worker              47h   v1.13.4
ip-172-31-29-132.compute.internal   Ready,SchedulingDisabled   worker              47h   v1.13.4
----


Now, go ahead and delete the MongoDB pod:

[source,bash]
----
$ POD=`kubectl get pods -l app=mongo -o wide | grep -v NAME | awk '{print $1}'`
$ kubectl delete pod ${POD}
pod "mongo-68cc69bc95-7q96h" deleted
----

When the pod is deleted, it is relocated to the node with the replicated data, even when that node is in a different zone.  
Each pod is rescheduled on the exact node where the data is stored.

Verify this:

[source,bash]
----
$ kubectl get pods -l app=mongo -o wide
NAME                     READY     STATUS    RESTARTS   AGE       IP               NODE
mongo-68cc69bc95-thqbm   1/1       Running   0          19s       192.168.82.119   ip-172-31-24-121.compute.internal
----

Note that a new pod has been created and scheduled in a different node.

Next, uncordon the node to bring it back to action:

[source,bash]
----
$ kubectl uncordon ${NODE}
node/ip-172-31-29-132.compute.internal uncordoned
----


Finally, verify that the data is still available.

To do so, find the pod name and run the ‘exec’ command, then access the Mongo shell:

[source,bash]
----
POD=`kubectl get pods -l app=mongo | grep Running | grep 1/1 | awk '{print $1}'`
kubectl exec -it $POD mongo
MongoDB shell version v4.0.0
connecting to: mongodb://127.0.0.1:27017
MongoDB server version: 4.0.0
Welcome to the MongoDB shell.
----


After that, query the collection to verify that the data is intact.

Find one arbitrary document:

[source,bash]
----
db.ships.findOne()
{
	"_id" : ObjectId("5b5c16221108c314d4c000cd"),
	"name" : "USS Enterprise-D",
	"operator" : "Starfleet",
	"type" : "Explorer",
	"class" : "Galaxy",
	"crew" : 750,
	"codes" : [
		10,
		11,
		12
	]
}
----


Now, find all documents and apply formatting:

[source,bash]
----
db.ships.find().pretty()
…..
{
	"_id" : ObjectId("5b5c16221108c314d4c000d1"),
	"name" : "IKS Somraw",
	"operator" : " Klingon Empire",
	"class" : "Raptor",
	"crew" : 50,
	"codes" : [
		101,
		111,
		120
	]
}
{
	"_id" : ObjectId("5b5c16221108c314d4c000d2"),
	"name" : "Scimitar",
	"operator" : "Romulan Star Empire",
	"type" : "Warbird",
	"class" : "Warbird",
	"crew" : 25,
	"codes" : [
		201,
		211,
		220
	]
}
----

For further validation, you can run all the same initial steps to compare that data is still available with originally queried values.
////


== Summary

////
MongoDB with SUSE Rancher makes it possible to easily build, deploy, and manage resilient, scalable data services.


== Additional resources

For more information, visit: 

*	https://rancher.com/docs/rancher/v2.x/en/best-practices/[Rancher best practices guide]
*	https://rancher.com/docs/rancher/v2.x/en/troubleshooting/[Rancher troubleshooting tips]
*	https://github.com/mongodb/mongodb-kubernetes-operator[MongoDB best practices]

++++
<?pdfpagebreak?>
++++

:leveloffset: 0
// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// include::common_gfdl1.2_i.adoc[]

:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end

////